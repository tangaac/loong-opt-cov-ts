	.file	"jfdctfst.c"
	.text
	.globl	jpeg_fdct_ifast                 # -- Begin function jpeg_fdct_ifast
	.p2align	5
	.type	jpeg_fdct_ifast,@function
jpeg_fdct_ifast:                        # @jpeg_fdct_ifast
# %bb.0:
	addi.w	$a2, $zero, -8
	ori	$a1, $zero, 181
	ori	$a3, $zero, 98
	ori	$a4, $zero, 139
	ori	$a5, $zero, 334
	move	$a6, $a0
	.p2align	4, , 16
.LBB0_1:                                # =>This Inner Loop Header: Depth=1
	ld.w	$a7, $a6, 0
	ld.w	$t0, $a6, 28
	ld.w	$t1, $a6, 4
	ld.w	$t2, $a6, 24
	add.d	$t3, $t0, $a7
	sub.d	$a7, $a7, $t0
	add.d	$t0, $t2, $t1
	ld.w	$t4, $a6, 8
	ld.w	$t5, $a6, 20
	ld.w	$t6, $a6, 12
	ld.w	$t7, $a6, 16
	sub.d	$t1, $t1, $t2
	add.d	$t2, $t5, $t4
	sub.d	$t4, $t4, $t5
	add.d	$t5, $t7, $t6
	sub.d	$t6, $t6, $t7
	add.d	$t7, $t5, $t3
	sub.d	$t3, $t3, $t5
	add.d	$t5, $t2, $t0
	sub.d	$t0, $t0, $t2
	add.d	$t2, $t7, $t5
	st.w	$t2, $a6, 0
	sub.d	$t2, $t7, $t5
	st.w	$t2, $a6, 16
	add.w	$t0, $t0, $t3
	mul.d	$t0, $t0, $a1
	srli.d	$t0, $t0, 8
	add.d	$t2, $t3, $t0
	st.w	$t2, $a6, 8
	sub.d	$t0, $t3, $t0
	st.w	$t0, $a6, 24
	add.w	$t0, $t6, $t4
	add.w	$t2, $t4, $t1
	add.w	$t1, $t1, $a7
	sub.w	$t3, $t0, $t1
	mul.d	$t3, $t3, $a3
	srli.d	$t3, $t3, 8
	mul.d	$t0, $t0, $a4
	srli.d	$t0, $t0, 8
	add.d	$t0, $t3, $t0
	mul.d	$t1, $t1, $a5
	srli.d	$t1, $t1, 8
	add.d	$t1, $t3, $t1
	mul.d	$t2, $t2, $a1
	srli.d	$t2, $t2, 8
	add.d	$t3, $a7, $t2
	sub.d	$a7, $a7, $t2
	add.d	$t2, $t0, $a7
	st.w	$t2, $a6, 20
	sub.d	$a7, $a7, $t0
	st.w	$a7, $a6, 12
	add.d	$a7, $t1, $t3
	st.w	$a7, $a6, 4
	sub.d	$a7, $t3, $t1
	st.w	$a7, $a6, 28
	bstrpick.d	$a2, $a2, 31, 0
	addi.d	$a2, $a2, 1
	slli.d	$a7, $a2, 31
	addi.d	$a6, $a6, 32
	bgez	$a7, .LBB0_1
# %bb.2:                                # %vector.body
	vld	$vr0, $a0, 0
	vld	$vr1, $a0, 224
	vld	$vr2, $a0, 32
	vld	$vr3, $a0, 192
	vadd.w	$vr5, $vr1, $vr0
	vsub.w	$vr4, $vr0, $vr1
	vadd.w	$vr0, $vr3, $vr2
	vld	$vr1, $a0, 64
	vld	$vr6, $a0, 160
	vld	$vr7, $a0, 96
	vld	$vr8, $a0, 128
	vsub.w	$vr2, $vr2, $vr3
	vadd.w	$vr3, $vr6, $vr1
	vsub.w	$vr1, $vr1, $vr6
	vadd.w	$vr6, $vr8, $vr7
	vsub.w	$vr7, $vr7, $vr8
	vadd.w	$vr8, $vr6, $vr5
	vsub.w	$vr5, $vr5, $vr6
	vadd.w	$vr6, $vr3, $vr0
	vsub.w	$vr0, $vr0, $vr3
	vadd.w	$vr3, $vr8, $vr6
	vst	$vr3, $a0, 0
	vsub.w	$vr3, $vr8, $vr6
	vst	$vr3, $a0, 128
	vadd.w	$vr0, $vr0, $vr5
	vshuf4i.w	$vr3, $vr0, 16
	vslli.d	$vr3, $vr3, 32
	vsrai.d	$vr3, $vr3, 32
	vshuf4i.w	$vr0, $vr0, 50
	vslli.d	$vr0, $vr0, 32
	vsrai.d	$vr6, $vr0, 32
	vrepli.d	$vr0, 181
	vmul.d	$vr6, $vr6, $vr0
	vmul.d	$vr3, $vr3, $vr0
	vsrli.d	$vr3, $vr3, 8
	vsrli.d	$vr6, $vr6, 8
	vpickev.w	$vr3, $vr6, $vr3
	vadd.w	$vr6, $vr5, $vr3
	vst	$vr6, $a0, 64
	vsub.w	$vr3, $vr5, $vr3
	vst	$vr3, $a0, 192
	vadd.w	$vr3, $vr7, $vr1
	vadd.w	$vr5, $vr1, $vr2
	vadd.w	$vr6, $vr2, $vr4
	vsub.w	$vr1, $vr3, $vr6
	vshuf4i.w	$vr2, $vr1, 16
	vslli.d	$vr2, $vr2, 32
	vsrai.d	$vr2, $vr2, 32
	vshuf4i.w	$vr1, $vr1, 50
	vslli.d	$vr1, $vr1, 32
	vsrai.d	$vr7, $vr1, 32
	vrepli.d	$vr1, 98
	vmul.d	$vr7, $vr7, $vr1
	vmul.d	$vr2, $vr2, $vr1
	vsrli.d	$vr2, $vr2, 8
	vsrli.d	$vr7, $vr7, 8
	vpickev.w	$vr7, $vr7, $vr2
	vshuf4i.w	$vr2, $vr3, 16
	vslli.d	$vr2, $vr2, 32
	vsrai.d	$vr8, $vr2, 32
	vshuf4i.w	$vr2, $vr3, 50
	vslli.d	$vr2, $vr2, 32
	vsrai.d	$vr3, $vr2, 32
	vrepli.d	$vr2, 139
	vmul.d	$vr3, $vr3, $vr2
	vmul.d	$vr8, $vr8, $vr2
	vsrli.d	$vr8, $vr8, 8
	vsrli.d	$vr3, $vr3, 8
	vpickev.w	$vr3, $vr3, $vr8
	vadd.w	$vr8, $vr7, $vr3
	vshuf4i.w	$vr3, $vr6, 16
	vslli.d	$vr3, $vr3, 32
	vsrai.d	$vr9, $vr3, 32
	vshuf4i.w	$vr3, $vr6, 50
	vslli.d	$vr3, $vr3, 32
	vsrai.d	$vr6, $vr3, 32
	vrepli.d	$vr3, 334
	vmul.d	$vr6, $vr6, $vr3
	vmul.d	$vr9, $vr9, $vr3
	vsrli.d	$vr9, $vr9, 8
	vsrli.d	$vr6, $vr6, 8
	vpickev.w	$vr6, $vr6, $vr9
	vadd.w	$vr6, $vr7, $vr6
	vshuf4i.w	$vr7, $vr5, 16
	vslli.d	$vr7, $vr7, 32
	vsrai.d	$vr7, $vr7, 32
	vshuf4i.w	$vr5, $vr5, 50
	vslli.d	$vr5, $vr5, 32
	vsrai.d	$vr5, $vr5, 32
	vmul.d	$vr5, $vr5, $vr0
	vmul.d	$vr7, $vr7, $vr0
	vsrli.d	$vr7, $vr7, 8
	vsrli.d	$vr5, $vr5, 8
	vpickev.w	$vr5, $vr5, $vr7
	vadd.w	$vr7, $vr4, $vr5
	vsub.w	$vr4, $vr4, $vr5
	vadd.w	$vr5, $vr8, $vr4
	vst	$vr5, $a0, 160
	vsub.w	$vr4, $vr4, $vr8
	vst	$vr4, $a0, 96
	vadd.w	$vr4, $vr6, $vr7
	vst	$vr4, $a0, 32
	vsub.w	$vr4, $vr7, $vr6
	vld	$vr5, $a0, 16
	vld	$vr6, $a0, 240
	vld	$vr7, $a0, 48
	vld	$vr8, $a0, 208
	vst	$vr4, $a0, 224
	vadd.w	$vr9, $vr6, $vr5
	vsub.w	$vr4, $vr5, $vr6
	vadd.w	$vr5, $vr8, $vr7
	vld	$vr6, $a0, 80
	vld	$vr10, $a0, 176
	vld	$vr11, $a0, 112
	vld	$vr12, $a0, 144
	vsub.w	$vr7, $vr7, $vr8
	vadd.w	$vr8, $vr10, $vr6
	vsub.w	$vr6, $vr6, $vr10
	vadd.w	$vr10, $vr12, $vr11
	vsub.w	$vr11, $vr11, $vr12
	vadd.w	$vr12, $vr10, $vr9
	vsub.w	$vr9, $vr9, $vr10
	vadd.w	$vr10, $vr8, $vr5
	vsub.w	$vr5, $vr5, $vr8
	vadd.w	$vr8, $vr12, $vr10
	vst	$vr8, $a0, 16
	vsub.w	$vr8, $vr12, $vr10
	vst	$vr8, $a0, 144
	vadd.w	$vr5, $vr5, $vr9
	vshuf4i.w	$vr8, $vr5, 16
	vslli.d	$vr8, $vr8, 32
	vsrai.d	$vr8, $vr8, 32
	vshuf4i.w	$vr5, $vr5, 50
	vslli.d	$vr5, $vr5, 32
	vsrai.d	$vr5, $vr5, 32
	vmul.d	$vr5, $vr5, $vr0
	vmul.d	$vr8, $vr8, $vr0
	vsrli.d	$vr8, $vr8, 8
	vsrli.d	$vr5, $vr5, 8
	vpickev.w	$vr5, $vr5, $vr8
	vadd.w	$vr8, $vr9, $vr5
	vst	$vr8, $a0, 80
	vsub.w	$vr5, $vr9, $vr5
	vst	$vr5, $a0, 208
	vadd.w	$vr5, $vr11, $vr6
	vadd.w	$vr6, $vr6, $vr7
	vadd.w	$vr7, $vr7, $vr4
	vsub.w	$vr8, $vr5, $vr7
	vshuf4i.w	$vr9, $vr8, 16
	vslli.d	$vr9, $vr9, 32
	vsrai.d	$vr9, $vr9, 32
	vshuf4i.w	$vr8, $vr8, 50
	vslli.d	$vr8, $vr8, 32
	vsrai.d	$vr8, $vr8, 32
	vmul.d	$vr8, $vr8, $vr1
	vmul.d	$vr1, $vr9, $vr1
	vsrli.d	$vr1, $vr1, 8
	vsrli.d	$vr8, $vr8, 8
	vpickev.w	$vr1, $vr8, $vr1
	vshuf4i.w	$vr8, $vr5, 16
	vslli.d	$vr8, $vr8, 32
	vsrai.d	$vr8, $vr8, 32
	vshuf4i.w	$vr5, $vr5, 50
	vslli.d	$vr5, $vr5, 32
	vsrai.d	$vr5, $vr5, 32
	vmul.d	$vr5, $vr5, $vr2
	vmul.d	$vr2, $vr8, $vr2
	vsrli.d	$vr2, $vr2, 8
	vsrli.d	$vr5, $vr5, 8
	vpickev.w	$vr2, $vr5, $vr2
	vadd.w	$vr2, $vr1, $vr2
	vshuf4i.w	$vr5, $vr7, 16
	vslli.d	$vr5, $vr5, 32
	vsrai.d	$vr5, $vr5, 32
	vshuf4i.w	$vr7, $vr7, 50
	vslli.d	$vr7, $vr7, 32
	vsrai.d	$vr7, $vr7, 32
	vmul.d	$vr7, $vr7, $vr3
	vmul.d	$vr3, $vr5, $vr3
	vsrli.d	$vr3, $vr3, 8
	vsrli.d	$vr5, $vr7, 8
	vpickev.w	$vr3, $vr5, $vr3
	vadd.w	$vr1, $vr1, $vr3
	vshuf4i.w	$vr3, $vr6, 16
	vslli.d	$vr3, $vr3, 32
	vsrai.d	$vr3, $vr3, 32
	vshuf4i.w	$vr5, $vr6, 50
	vslli.d	$vr5, $vr5, 32
	vsrai.d	$vr5, $vr5, 32
	vmul.d	$vr5, $vr5, $vr0
	vmul.d	$vr0, $vr3, $vr0
	vsrli.d	$vr0, $vr0, 8
	vsrli.d	$vr3, $vr5, 8
	vpickev.w	$vr0, $vr3, $vr0
	vadd.w	$vr3, $vr4, $vr0
	vsub.w	$vr0, $vr4, $vr0
	vadd.w	$vr4, $vr2, $vr0
	vst	$vr4, $a0, 176
	vsub.w	$vr0, $vr0, $vr2
	vst	$vr0, $a0, 112
	vadd.w	$vr0, $vr1, $vr3
	vst	$vr0, $a0, 48
	vsub.w	$vr0, $vr3, $vr1
	vst	$vr0, $a0, 240
	ret
.Lfunc_end0:
	.size	jpeg_fdct_ifast, .Lfunc_end0-jpeg_fdct_ifast
                                        # -- End function
	.section	".note.GNU-stack","",@progbits
	.addrsig
